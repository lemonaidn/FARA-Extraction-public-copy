{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that genai is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google.generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your API key and all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "# Load API Key File\n",
    "###\n",
    "import secret_api_key_file\n",
    "\n",
    "###\n",
    "# Load Google's LLM package\n",
    "###\n",
    "import google.generativeai as genai\n",
    "\n",
    "###\n",
    "# Import other stuff we'll need\n",
    "###\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "###\n",
    "# Store your API key\n",
    "###\n",
    "\n",
    "genai.configure(api_key=secret_api_key_file.gemini_api_key)\n",
    "\n",
    "###\n",
    "# Store the model we'll use for this: 1.5 Flash\n",
    "# Full model list: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "###\n",
    "\n",
    "#Create a model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV file that contains the urls for all short form disclosures between 2014 and present day in 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "csv_file_path = r\"C:\\Users\\Aidan\\Documents\\GitHub\\fara_project_group_repo\\short_form_14_to_24.csv\"\n",
    "short_form_df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the first 100 urls and extract the filer's responses to sections 10-12.\n",
    "\n",
    "Ensure the output doesn't contain the questions for each section -- we only want to preserve the unique text / the responses from each form.\n",
    "\n",
    "Also ensure that the model isn't thrown off by the two-part questions of section 12 (the yes/no checkbox followed by the \"if yes...\" part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Create a temporary folder if you didn't already run the prior step\n",
    "temp_folder = os.path.join(\"./\", \"temp\")\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "# Function to download a file from a URL\n",
    "def download_file(url, folder):\n",
    "    local_filename = os.path.join(folder, \"temp_file.pdf\")\n",
    "    with requests.get(url, stream=True, verify=False) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    return local_filename\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through the first 100 URLs\n",
    "for url in short_form_df[\"URL\"][:100]:\n",
    "    # Delete any existing file in the temporary folder\n",
    "    for filename in os.listdir(temp_folder):\n",
    "        file_path = os.path.join(temp_folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "    \n",
    "    # Download the new file\n",
    "    try:\n",
    "        file_path = download_file(url, temp_folder)\n",
    "        print(f\"Downloaded and saved file from {url}\")\n",
    "        \n",
    "        # Upload the file to Google's servers\n",
    "        sample_short_form = genai.upload_file(file_path)\n",
    "        time.sleep(1)  # Ensure at least 1-second delay between uploads\n",
    "        \n",
    "        # Define the prompt\n",
    "        prompt = \"\"\"\n",
    "        Extract the text from the responses in sections 10-12 of the document. Don't add any additional commentary.\n",
    "\n",
    "        Ignore the yes or no box in section 12. Only provide the text response to the question if there is one.\n",
    "\n",
    "        Remove the following to preserve only the responses to the form's prompts and the numbers of the sections:\n",
    "\n",
    "        - \"List the foreign principal to whom you will render services in support of the primary registrant.\"\n",
    "        - \"Describe in detail all services which you will render to the foreign principal listed in Item 10 either directly, or through the primary registrant listed in Item 8.\"\n",
    "        - \"Do any of the above-described services include political activity as defined in Section l(o) of the Act* 1? If yes, describe separately and in detail such political activity. The response must include, but not be limited to, activities involving lobbying, promotion, perception management, public relations, economic development, and preparation or dissemination of informational materials.\"\n",
    "\n",
    "        Format this all as structured json.\n",
    "\n",
    "        Any empty sections should be left as \"None\"\n",
    "\n",
    "        This is the structure of the json you should return:\n",
    "\n",
    "        ```json\n",
    "        [\n",
    "        {\n",
    "            \"10.\": \"example text\",\n",
    "            \"11.\": \"example text\",\n",
    "            \"12.\": \"example text\",\n",
    "        },\n",
    "\n",
    "        here's another example:\n",
    "\n",
    "        ```json\n",
    "        [\n",
    "        {\n",
    "            \"10.\": \"example text\",\n",
    "            \"11.\": \"example text\",\n",
    "            \"12.\": \"None\",\n",
    "        },\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run the file through Gemini\n",
    "        response = model.generate_content([prompt, sample_short_form])\n",
    "        time.sleep(1)  # Ensure at least 1-second delay between requests\n",
    "        \n",
    "        # Ensure response.text is as expected and strip code block markers\n",
    "        response_text = response.text.strip()\n",
    "        if response_text.startswith(\"```json\"):\n",
    "            response_text = response_text[7:-3].strip()  # Remove ```json and ```\n",
    "        \n",
    "        # Now attempt to parse the cleaned JSON\n",
    "        try:\n",
    "            # Parse JSON\n",
    "            json_data = json.loads(response_text)\n",
    "            \n",
    "            # Convert JSON to DataFrame\n",
    "            df = pd.DataFrame(json_data)\n",
    "\n",
    "            # Add the URL column\n",
    "            df.insert(0, \"URL\", url)\n",
    "            \n",
    "            # Display the DataFrame\n",
    "            print(\"DataFrame created successfully.\")\n",
    "            display(df)  # Use display(df) in Jupyter notebooks to render nicely\n",
    "            \n",
    "            # Store the result\n",
    "            results.append({\n",
    "                \"URL\": url,\n",
    "                \"Output\": df\n",
    "            })\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON parsing error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or process file from {url}: {e}\")\n",
    "\n",
    "# Save the results to a CSV file\n",
    "section_10_to_12_results_df = pd.concat([result[\"Output\"] for result in results], ignore_index=True)\n",
    "section_10_to_12_results_df_path = os.path.join(\"./\", \"section_10_to_12_results.csv\")\n",
    "section_10_to_12_results_df.to_csv(section_10_to_12_results_df_path, index=False)\n",
    "\n",
    "print(f\"Results saved to {section_10_to_12_results_df_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do a second pass focusing on extracting the text from any appendices in the file. We're doing this as a separate processing step from our work above to limit the number of tasks we're asking Gemini to do in any given prompt. This has significantly improved accuracy versus a single all-inclusive prompt.\n",
    "\n",
    "In post-processing outside Gemini further down, we'll whittle this down to only sections 10-12. But The output is more accurate with the prompt below, which captures all appendices, than when we try to get it to search only for appendices pertaining to sections 10-12. Better to capture too much info and whittle down later, than not enough or inaccurate info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Create a temporary folder if you didn't already run the prior step\n",
    "temp_folder = os.path.join(\"./\", \"temp\")\n",
    "os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "# Function to download a file from a URL\n",
    "def download_file(url, folder):\n",
    "    local_filename = os.path.join(folder, \"temp_file.pdf\")\n",
    "    with requests.get(url, stream=True, verify=False) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    return local_filename\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# DataFrame to store extracted text and corresponding URLs\n",
    "extracted_data = pd.DataFrame(columns=[\"URL\", \"Extracted_Text\"])\n",
    "\n",
    "# Iterate through the first 100 URLs\n",
    "for url in short_form_df[\"URL\"][:100]:\n",
    "    # Delete any existing file in the temporary folder\n",
    "    for filename in os.listdir(temp_folder):\n",
    "        file_path = os.path.join(temp_folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "\n",
    "    # Download the new file\n",
    "    try:\n",
    "        file_path = download_file(url, temp_folder)\n",
    "        print(f\"Downloaded and saved file from {url}\")\n",
    "\n",
    "        # Upload the file to Google's servers\n",
    "        sample_short_form = genai.upload_file(file_path)\n",
    "\n",
    "        # Define the prompt\n",
    "        prompt = \"\"\"\n",
    "        Please extract all the text from the provided PDF file for the appendix page, which has the word \"Appendix\" in the page's header. Ensure that any handwritten content is also included to the best of your ability.\n",
    "\n",
    "        Don't extract information from any page that doesn't have the word \"Appendix\" in the header.\n",
    "        \n",
    "        Structure it as JSON.\n",
    "\n",
    "        If there is no appendix page, all columns should be blank but the column headers shown in the example output below MUST still be there.\n",
    "\n",
    "        Here is an example output format:\n",
    "\n",
    "        [\n",
    "            {\n",
    "                \"Appendix response to Item 1\": \"None\",\n",
    "                \"Appendix response to Item 2\": \"None\",\n",
    "                \"Appendix response to Item 3\": \"Item 3: Residence Address. Broumana Main Road, Vermont Building, Ground Floor, 2753 Ain Saade, Metn LEBANON\",\n",
    "                \"Appendix response to Item 4\": \"None\",\n",
    "                \"Appendix response to Item 5\": \"None\",\n",
    "                \"Appendix response to Item 6\": \"None\",\n",
    "                \"Appendix response to Item 7\": \"None\",\n",
    "                \"Appendix response to Item 8\": \"None\",\n",
    "                \"Appendix response to Item 9\": \"None\",\n",
    "                \"Appendix response to Item 10\": \"None\",\n",
    "                \"Appendix response to Item 11\": \"None\",\n",
    "                \"Appendix response to Item 12\": \"None\",\n",
    "                \"Appendix response to Item 13\": \"None\",\n",
    "                \"Appendix response to Item 14\": \"None\",\n",
    "                \"Appendix response to Item 15\": \"None\"\n",
    "            }\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the file through Gemini\n",
    "        response = model.generate_content([prompt, sample_short_form])\n",
    "\n",
    "        # Ensure response.text is as expected and strip code block markers\n",
    "        response_text = response.text.strip()\n",
    "        if response_text.startswith(\"```json\"):\n",
    "            response_text = response_text[7:-3].strip()  # Remove ```json and ```\n",
    "\n",
    "        # Now attempt to parse the cleaned JSON\n",
    "        try:\n",
    "            # Parse JSON\n",
    "            json_data = json.loads(response_text)\n",
    "\n",
    "            # Convert JSON to DataFrame\n",
    "            df = pd.DataFrame(json_data)\n",
    "\n",
    "            # Add the URL to the DataFrame and move it to the first column\n",
    "            df.insert(0, \"URL\", url)\n",
    "\n",
    "            # Display the DataFrame\n",
    "            print(\"DataFrame created successfully.\")\n",
    "            display(df)  # Use display(df) in Jupyter notebooks to render nicely\n",
    "\n",
    "            # Store the result\n",
    "            results.append(df)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON parsing error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or process file from {url}: {e}\")\n",
    "\n",
    "# Concatenate all results into a single DataFrame\n",
    "results_appendices_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Define the desired save path\n",
    "save_folder = \"./\"\n",
    "results_appendices_path = os.path.join(save_folder, \"results_appendices.csv\")\n",
    "\n",
    "# Save the results to the specified folder\n",
    "results_appendices_df.to_csv(results_appendices_path, index=False)\n",
    "\n",
    "print(f\"Results saved to {results_appendices_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the two dataframes and only keep the relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join results_appenices_df with results_10_to_12_df on URL\n",
    "merged_df = pd.merge(results_appendices_df, section_10_to_12_results_df, on=\"URL\", how=\"inner\")\n",
    "\n",
    "# Keep only the specified columns in the final DataFrame\n",
    "final_columns = [\"URL\", \"10.\", \"11.\", \"12.\", \"Appendix response to Item 10\", \"Appendix response to Item 11\", \"Appendix response to Item 12\"]\n",
    "final_df = merged_df[final_columns]\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(\"Final DataFrame:\")\n",
    "display(final_df)\n",
    "\n",
    "# Save the final DataFrame to a CSV file in the specified directory\n",
    "final_results_path = r\"C:\\Users\\Aidan\\Documents\\GitHub\\fara_project_group_repo\\final_results.csv\"\n",
    "final_df.to_csv(final_results_path, index=False)\n",
    "\n",
    "print(f\"Final results saved to {final_results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
